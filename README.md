# ğŸ§  Adversarial Attacks Comparison

> **Experimental comparison of FGSM, PGD, and DeepFool â€” analyzing trade-offs between performance, perceptibility, and transferability**

---

## ğŸ“˜ Overview

This project implements and compares three classical adversarial attack methods â€” **FGSM**, **PGD**, and **DeepFool** â€” using **PyTorch**.  
Experiments are conducted on the **MNIST** and **CIFAR-10** datasets.

Our goal is to provide a **clear, reproducible, and interpretable** experimental study of how different attack strategies impact model robustness, visual perturbation quality, and computational cost.

Deliverables include:
- Clean, modular, and reproducible code
- Well-documented Jupyter notebooks
- Visual and quantitative results (accuracy vs Îµ, perturbation norms, transferability matrix)
- A concise **technical report** and **presentation slides**

This repository demonstrates practical ML and research engineering skills:
> **PyTorch implementation, experimental design, evaluation, visualization, and robust model analysis.**

---

## ğŸ—‚ Table of Contents

1. [Background & Motivation](#background--motivation)
2. [Project Objectives](#project-objectives)
3. [Implementation Plan (Aâ€“Z)](#implementation-plan-aâ€“z)
4. [Expected Results & Analysis](#expected-results--analysis)

---

## ğŸ” Background & Motivation

Deep neural networks are vulnerable to small, human-imperceptible perturbations known as **adversarial examples** â€” tiny pixel changes that can cause large misclassifications.  
These vulnerabilities pose critical challenges for **security, reliability, and interpretability** in ML systems.

Rather than training a noise generator (as other teams might do), our project performs a **systematic and reproducible comparison** of standard white-box attacks to highlight practical **trade-offs** between:

- ğŸ§© **Attack strength:** impact on model accuracy  
- ğŸ‘ï¸ **Perturbation visibility:** measured with Lâˆ and L2 norms  
- âš™ï¸ **Computation cost:** time per image / per batch  
- ğŸ” **Transferability:** how well adversarial samples fool other architectures  

This focus on reproducibility and interpretability makes the project useful for both **academic study** and **real-world ML reliability** analysis.

---

## ğŸ¯ Project Objectives

### Core Goals
- **Implement** three well-known adversarial attacks using PyTorch:
  - `FGSM` (Fast Gradient Sign Method)
  - `PGD` (Projected Gradient Descent)
  - `DeepFool`
- **Evaluate** each attack in terms of:
  - Model accuracy (clean vs attacked)
  - Perturbation norms (Lâˆ, L2)
  - Average generation time per image
  - Transferability between architectures
- **Test** on:
  - **MNIST** â†’ lightweight, visual debugging
  - **CIFAR-10** â†’ realistic, more complex setup
- **Deliver** professional and reproducible materials:
  - Modular code, organized notebooks, plots, report, and slides

### Optional Extensions
- Implement **Adversarial Training** (e.g., FGSM-based)  
  â†’ Study how including adversarial samples during training affects robustness and cost.

---

## âš™ï¸ Implementation Plan (Aâ€“Z)

### ğŸ§© Environment Setup
- Create a Python environment (`conda` or `venv`)  
- Dependencies:
  ```bash
  torch
  torchvision
  numpy
  pandas
  matplotlib
  tqdm
  scikit-image
